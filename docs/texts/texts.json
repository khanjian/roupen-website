[
  {
    "path": "texts/2021-02-25-parks-and-recreation-text-analysis/",
    "title": "Parks and Recreation Text Analysis",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "roupen khanjian",
        "url": {}
      }
    ],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\nData Introduction\nParks and recreation was a television comedy show that aired on NBC from 2009 until 2015. I obtained the complete transcripts and performed text analysis on the dialogue of the show.\nCitation for dataset: He, Luke. (2019, November 23) Park and Recreation Scripts. Link to data.\n\n\nfile_names <- list.files(here(\"_texts\", \n                              \"2021-02-25-parks-and-recreation-text-analysis\", \n                              \"scripts\")) # file names for each episode\n\nparks <- str_glue(\"scripts/{file_names}\") %>% \n  map_dfr(read_csv) # read in all the episodes into one data frame!\n\n# Tokenize lines to one word in each row\nparks_token <- parks %>% \n  clean_names() %>% \n  unnest_tokens(word, line) %>% # tokenize\n  anti_join(stop_words) %>% # remove stop words\n  mutate(word = str_extract(word, \"[a-z']+\")) %>% # extract words only\n  drop_na(word) # take out missing values\n\n# Filter the top 10 characters with the most words\ntop_characters <- parks_token %>%\n  dplyr::filter(character != \"Extra\") %>% \n  count(character, sort = TRUE) %>%\n  slice_max(n, n = 10) \n\n# Obtain words only from the top 10 characters\nparks_words <- parks_token %>% \n  inner_join(top_characters) %>% \n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>% \n  select(-n) %>% \n  count(word, character, sort = TRUE) %>% \n  ungroup() %>% \n  group_by(character) %>% \n  top_n(9) # top 9\n\n# Sample of a few lines from the show\nparks %>% \n  slice(sample(1:65942, 20)) %>% \n  kbl(caption = \"<b style = 'color:white;'>\n       Sample of a few randomly chosen lines from Parks and Recreation.\") %>%\n  kable_material_dark(bootstrap_options = c(\"striped\", \"hover\")) %>%\n  row_spec(0, color = \"white\", background = \"#222222\") %>%\n  scroll_box(width = \"100%\", height = \"300px\", \n             fixed_thead = list(enabled = T, background = \"#222222\"))\n\n\n\n\nTable 1:  Sample of a few randomly chosen lines from Parks and Recreation.\n\n\nCharacter\n\n\nLine\n\n\nLeslie Knope\n\n\nNo. \n\n\nLeslie Knope\n\n\nBut I have to say, there’s a very sweet aftertaste though.\n\n\nRon Swanson\n\n\nMy first day of college, my father dropped me off at the steel mill.\n\n\nJessica Wicks\n\n\nIs Oprah involved in your bid?\n\n\nTom Haverford\n\n\nYou read books all the time.\n\n\nAndy Dwyer\n\n\nThis weekend, guys.\n\n\nAnn Perkins\n\n\nWe all chipped in.\n\n\nZoe Lewis\n\n\nThanks, Ann.\n\n\nLeslie Knope\n\n\nJust let me in.\n\n\nTom Haverford\n\n\nNo, it’s a spare room I converted into a walk-in closet/ home fitness center.\n\n\nLeslie Knope\n\n\nPawnee raccoon attacks have decreased.\n\n\nRon Swanson\n\n\nI’ve personally put out several local fires at no cost to the taxpayer.\n\n\nChris Traeger\n\n\nWell, it’s true.\n\n\nDiane Lewis\n\n\nCan we talk? Maybe in private?\n\n\nRon Swanson\n\n\nMy dream is to have the park system privatized and run entirely for profit by corporations.\n\n\nChris Traeger\n\n\nWhy do you want to achieve this goal?\n\n\nLeslie Knope\n\n\nI also made these: Leslie’s toffee surprise.\n\n\nAndy Dwyer\n\n\nI’ve always wanted one of these.\n\n\nDonna Meagle\n\n\nYou’ll probably never guess what kind of dog I am.\n\n\nJustin\n\n\nSo he freaked out a little bit.\n\n\n\nWord Count of Major Characters\nIt’s difficult to choose a favorite character from Parks and Rec, thus I plotted the top 9 most frequently used words from ten characters. Some examples of words that would resonate with fans of the show are Chris Traeger’s literally, Jerry (Gary) Gergich’s geez, or Ben Wyatt’s uh.\n\n\nggplot(data = parks_words, \n       aes(x = n, y = word, fill = n)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  facet_wrap(~character, scales = \"free\") +\n  theme_brooklyn99() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(size = 9),\n        axis.text.y = element_text(size = 9.5),\n        axis.title = element_blank(),\n        panel.grid.minor = element_blank(),\n        strip.text = element_text(color = \"white\",\n                                  face = \"bold\",\n                                  size = 10.5))\n\n\n\n\nWordcloud\nBelow are four wordclouds of the 25 most frequently used words by the following characters starting from the upper left hand corner going clockwise: Andy Dwyer, April Ludgate, Ron Swanson, and Leslie Knope. We can see Andy Dwyer’s enthusiasm with karate and band, Leslie Knope’s love for pawnee, city, and parks, but also Ron Swanson’s contempt for government and his 2 ex-wives both named tammy.\n\n\n# Ron Swanson\n\nswanson_words <- parks_token %>% \n  filter(character == \"Ron Swanson\") %>% # filter for character\n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>% # remove some more stopwords\n  count(word) %>% \n  slice_max(n,n = 25) # choose top 25 words\n  \nswanson_pic <- jpeg::readJPEG(here(\"_texts\",\n                                   \"2021-02-25-parks-and-recreation-text-analysis\",\n                                   \"images\",\n                                   \"ron_swanson.jpg\")) \n\nswanson_cloud <- ggplot(data = swanson_words,\n                        aes(label = word)) +\n  background_image(swanson_pic) + # add image of character\n  geom_text_wordcloud(aes(size = n), \n                      color = \"turquoise1\",\n                      shape = \"circle\") +\n  scale_size_area(max_size = 6) +\n  theme_void()\n\n# Lesile Knope\n\nknope_words <- parks_token %>% \n  filter(character == \"Leslie Knope\") %>% \n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>% # remove some more stopwords\n  count(word) %>% \n  slice_max(n,n = 25)\n  \nknope_pic <- jpeg::readJPEG(here(\"_texts\",\n                                 \"2021-02-25-parks-and-recreation-text-analysis\",\n                                 \"images\", \n                                 \"knope.jpg\"))\n\nknope_cloud <- ggplot(data = knope_words,\n                        aes(label = word)) +\n  background_image(knope_pic) +\n  geom_text_wordcloud(aes(size = n), \n                      color = \"turquoise1\",\n                      shape = \"star\") +\n  scale_size_area(max_size = 6) +\n  theme_void()\n\n# April Ludgate\n\napril_words <- parks_token %>% \n  filter(character == \"April Ludgate\") %>% \n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>% # remove some more stopwords\n  count(word) %>% \n  slice_max(n,n = 25)\n  \napril_pic <- jpeg::readJPEG(here(\"_texts\",\n                                 \"2021-02-25-parks-and-recreation-text-analysis\",\n                                 \"images\", \n                                 \"april.jpeg\"))\n\napril_cloud <- ggplot(data = april_words,\n                        aes(label = word)) +\n  background_image(april_pic) +\n  geom_text_wordcloud(aes(size = n), \n                      color = \"turquoise1\",\n                      shape = \"triangle-upright\") +\n  scale_size_area(max_size = 6) +\n  theme_void()\n\n# Andy Dwyer\n\nandy_words <- parks_token %>% \n  filter(character == \"Andy Dwyer\") %>% \n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>% # remove some more stopwords\n  count(word) %>% \n  slice_max(n,n = 25)\n  \nandy_pic <- jpeg::readJPEG(here(\"_texts\",\n                                \"2021-02-25-parks-and-recreation-text-analysis\",\n                                \"images\", \n                                \"andy.jpg\"))\n\nandy_cloud <- ggplot(data = andy_words,\n                        aes(label = word)) +\n  background_image(andy_pic) +\n  geom_text_wordcloud(aes(size = n), \n                      color = \"turquoise1\",\n                      shape = \"diamond\") +\n  scale_size_area(max_size = 6) +\n  theme_void()\n\n# Final patcwork wordcloud\n\npatchwork <-  (andy_cloud + april_cloud) / (knope_cloud + swanson_cloud) \n\npatchwork & theme(plot.background = element_rect(fill = \"#222222\",\n                                                 color = \"#222222\"),\n                  strip.background = element_rect(fill = \"#222222\",\n                                                 color = \"#222222\"))\n\n\n\n\nCharacter Sentimemnt Analysis\nUsing the nrc lexicon, which bins 13,901 words into 8 emotions, along with giving them a positive or negative rating, I plotted the counts of each sentiment for ten characters. We see that all the characters shown here use more positive words, and they all used words associated with trust and anticipation.\nCitation for NRC lexicon: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013. nrc lexicon\n\n\ncharacters_sent <-  parks_token %>%\n  inner_join(top_characters) %>%\n  filter(!word %in% c(\"hey\", \"yeah\", \"gonna\")) %>%\n  select(-n) %>% \n  inner_join(get_sentiments(\"nrc\")) %>% \n  count(sentiment, character, sort = TRUE)\n\nggplot(data = characters_sent, \n       aes(x = n, y = sentiment, fill = n)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  facet_wrap(~character, scales = \"free\") +\n  theme_brooklyn99() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(size = 7.5),\n        axis.text.y = element_text(size = 9.5),\n        axis.title = element_blank(),\n        panel.grid.minor = element_blank(),\n        strip.text = element_text(color = \"white\",\n                                  face = \"bold\",\n                                  size = 9.3))\n\n\n\n\nTrajectory of Sentiment\nParks and Recreation is a hilarious comedy show with many enjoyable characters. Thus, it’s no surprise that for most of the show the average sentiment is more positive. Using the AFINN lexicon, which assigns words a score between -5 (negative sentiment) and 5 (positive sentiment), I obtained the moving average with a window size of 151, and plotted the moving average sentiment throughout the entirety of the show.\nCitation for AFINN lexicon: AFINN, Nielson, Finn Årup. Informatics and Mathematical Modelling, Technical University of Denmark. March 2011. AFINN lexicon\n\n\nparks_afinn <- parks_token %>% \n  inner_join(get_sentiments(\"afinn\")) %>%\n  drop_na(value) %>% \n  mutate(index = seq(1, length(word) ,1)) %>% # make an index\n  mutate(moving_avg = as.numeric(slide(value, # get moving average\n                                       mean, \n                                       .before = (151 - 1)/2 , \n                                       .after = (151 - 1)/2 ))) %>% \n  mutate(neg_pos = factor(case_when(\n    moving_avg > 0 ~ \"Positive\",\n    moving_avg <= 0 ~ \"Negative\"\n  )))\n\nsent_plot <- ggplot(data = parks_afinn, aes(x = index, y = moving_avg)) +\n  geom_col(aes(fill = neg_pos)) +\n  scale_fill_manual(values = c(\"Positive\" = \"springgreen2\",\n                               \"Negative\" = \"darkred\"))+\n  theme_minimal() +\n  labs(x = \"Index\",\n       y = \"Moving Average AFINN Sentiment\",\n       fill = \"\") +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 11,\n                                   face = \"bold\",\n                                   color = \"white\"),\n        axis.title.y = element_text(color = \"white\",\n                                  size = 12,\n                                  face = \"bold\"),\n        axis.title.x = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.background = element_rect(fill = \"#222222\", \n                                       color = \"#222222\"),\n        strip.background = element_rect(fill = \"#222222\", \n                                        color = \"#222222\"),\n        legend.text = element_text(color = \"white\",\n                                  size = 11,\n                                  face = \"bold\"))\n\nsent_plot\n\n\n\n\nSentiment Anaylsis of Season 4\nI decided to take a closer look at the sentiment throughout season 4 since this was one of the more popular seasons, where Leslie Knope is campaigning to be a member of the city council of Pawnee, Indiana. Here I used a moving average window of 51 to plot the AFINN sentiment value. We see that for most of the season the overall average sentiment is positive, except for a noticeable drop near the end of the season where the sentiment score falls around -1.\n\n\nfile_names_season <- str_sub(file_names, start = 3L)\n\n# used this line of code to easily find the episode number of each season\n# which(file_names_season == \"e01.csv\")\n\nseason_4 <- str_glue(\"scripts/{file_names[47:68]}\") %>% \n  map_dfr(read_csv)\n\n# Tokenize lines to one word in each row\nseason_token <-  season_4 %>% \n  clean_names() %>% \n  unnest_tokens(word, line) %>% # tokenize\n  anti_join(stop_words) %>% # remove stop words\n  mutate(word = str_extract(word, \"[a-z']+\")) %>% # extract words only\n  drop_na(word) # take out missing values\n\nseason_afinn <- season_token %>% \n  inner_join(get_sentiments(\"afinn\")) %>%\n  drop_na(value) %>% \n  mutate(index = seq(1, length(word) ,1)) %>% \n  mutate(moving_avg = as.numeric(slide(value,\n                                       mean, \n                                       .before = (51 - 1)/2 , \n                                       .after = (51 - 1)/2 ))) \n\n\nseason_plot <- ggplot(data = season_afinn, aes(x = index, y = moving_avg)) +\n  geom_col(aes(fill = moving_avg)) +\n  # scale_fill_distiller(type = \"div\",\n  #                      palette = \"GnPR\")+\n  scale_fill_carto_c(type = \"diverging\",\n                     palette = \"Earth\") +\n  theme_minimal() +\n  labs(x = \"Index\",\n       y = \"Moving Average AFINN Sentiment\",\n       fill = \"\") +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 11,\n                                   face = \"bold\",\n                                   color = \"white\"),\n        axis.title.y = element_text(color = \"white\",\n                                  size = 12,\n                                  face = \"bold\"),\n        axis.title.x = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.background = element_rect(fill = \"#222222\", \n                                       color = \"#222222\"),\n        strip.background = element_rect(fill = \"#222222\", \n                                        color = \"#222222\"),\n        legend.text = element_text(color = \"white\",\n                                  size = 11,\n                                  face = \"bold\"))\n\nseason_plot\n\n\n\n\nDigging into the data I found that this occurred during the penultimate episode of the season named “Bus Tour”. The episode starts with Lesile Knope behind in polls to her opponent in the city council race, Bobby Newport. During one of her campaign stops, in response to a question by a reporter, Lesile starts saying disparaging things about Bobby’s father. After she is finished, the reporter informs Leslie her question was about if she had any comments about his death earlier in the day. Meanwhile, in order to get people to the polls, Lesile’s team trys to secure vans to transport possible voters. But Bobby Newport’s team has secured all the vans in the city. Thus, most of the episode is spent trying to do damage control for Lesile and her campaign team’s mishaps. Below are the words that have AFINN ratings during this dip in sentiment in season 4.\n\n\n# Investigate the negative dip of the plot\nseason_afinn_neg <- season_afinn %>% \n  filter(moving_avg < -0.75) %>% \n  slice(-c(1:2)) %>% \n  select(-index) %>% \n  rename('moving average' = moving_avg)\n\n# How I figured out which episode it was\nseason_4_subset <- season_4 %>% \n  filter(Character == \"Bill\") \n\n# Table of words\nseason_afinn_neg %>% \n  kbl(caption = \"<b style = 'color:white;'>\n       What was happening towards the end of season 4 of Park and Recreation when things went south?\") %>%\n  kable_material_dark(bootstrap_options = c(\"striped\", \"hover\")) %>%\n  row_spec(0, color = \"white\", background = \"#222222\") %>%\n  scroll_box(width = \"100%\", height = \"300px\", \n             fixed_thead = list(enabled = T, background = \"#222222\"))\n\n\n\n\nTable 2:  What was happening towards the end of season 4 of Park and Recreation when things went south?\n\n\ncharacter\n\n\nword\n\n\nvalue\n\n\nmoving average\n\n\nBill\n\n\ngrand\n\n\n3\n\n\n-0.7647059\n\n\nTom Haverford\n\n\ndemands\n\n\n-1\n\n\n-0.7647059\n\n\nTom Haverford\n\n\ncrying\n\n\n-2\n\n\n-0.8431373\n\n\nLeslie Knope\n\n\npromise\n\n\n1\n\n\n-0.8235294\n\n\nLeslie Knope\n\n\nstop\n\n\n-1\n\n\n-0.8823529\n\n\nLeslie Knope\n\n\nintimidating\n\n\n-2\n\n\n-0.8823529\n\n\nLeslie Knope\n\n\nbullying\n\n\n-2\n\n\n-0.9019608\n\n\nLeslie Knope\n\n\njerk\n\n\n-3\n\n\n-0.9019608\n\n\nLeslie Knope\n\n\nwrong\n\n\n-2\n\n\n-0.9019608\n\n\nLeslie Knope\n\n\ndied\n\n\n-3\n\n\n-0.8627451\n\n\nLeslie Knope\n\n\nsad\n\n\n-2\n\n\n-0.9215686\n\n\nExtra\n\n\nsad\n\n\n-2\n\n\n-0.9215686\n\n\nLeslie Knope\n\n\nbummer\n\n\n-2\n\n\n-0.8039216\n\n\nLeslie Knope\n\n\njerk\n\n\n-3\n\n\n-0.7647059\n\n\nPerd Hapley\n\n\nlove\n\n\n3\n\n\n-0.7647059\n\n\nJennifer Barkley\n\n\ncancel\n\n\n-1\n\n\n-0.7843137\n\n\nLeslie Knope\n\n\nemergency\n\n\n-2\n\n\n-0.8235294\n\n\nLeslie Knope\n\n\ntrust\n\n\n1\n\n\n-0.8431373\n\n\nLeslie Knope\n\n\ndied\n\n\n-3\n\n\n-0.9019608\n\n\nLeslie Knope\n\n\nawful\n\n\n-3\n\n\n-0.8823529\n\n\nLeslie Knope\n\n\ndied\n\n\n-3\n\n\n-0.7843137\n\n\nAnn Perkins\n\n\ndead\n\n\n-3\n\n\n-0.7843137\n\n\nAnn Perkins\n\n\njerk\n\n\n-3\n\n\n-0.8823529\n\n\nLeslie Knope\n\n\njerk\n\n\n-3\n\n\n-0.9411765\n\n\nLeslie Knope\n\n\npolluted\n\n\n-2\n\n\n-0.9803922\n\n\nBen Wyatt\n\n\nstop\n\n\n-1\n\n\n-1.0980392\n\n\nBen Wyatt\n\n\nstop\n\n\n-1\n\n\n-1.1372549\n\n\nAnn Perkins\n\n\nfine\n\n\n2\n\n\n-1.1568627\n\n\nAnn Perkins\n\n\nstop\n\n\n-1\n\n\n-1.1960784\n\n\nAnn Perkins\n\n\napologize\n\n\n-1\n\n\n-1.1960784\n\n\nChris Traeger\n\n\nworst\n\n\n-3\n\n\n-1.1764706\n\n\nChris Traeger\n\n\nstop\n\n\n-1\n\n\n-1.0980392\n\n\nChris Traeger\n\n\nstops\n\n\n-1\n\n\n-1.0588235\n\n\nChris Traeger\n\n\nstop\n\n\n-1\n\n\n-1.0392157\n\n\nChris Traeger\n\n\nstopping\n\n\n-1\n\n\n-0.9607843\n\n\nChris Traeger\n\n\ndeath\n\n\n-2\n\n\n-0.8627451\n\n\nLeslie Knope\n\n\nbeautiful\n\n\n3\n\n\n-0.8431373\n\n\nLeslie Knope\n\n\nclassy\n\n\n3\n\n\n-0.8235294\n\n\nDonna Meagle\n\n\nfree\n\n\n1\n\n\n-0.8235294\n\n\nDonna Meagle\n\n\nhuge\n\n\n1\n\n\n-0.7647059\n\n\nBill\n\n\nyeah\n\n\n1\n\n\n-0.8627451\n\n\nBill\n\n\nhell\n\n\n-4\n\n\n-0.8039216\n\n\nBill\n\n\nfree\n\n\n1\n\n\n-0.8039216\n\n\nBill\n\n\npay\n\n\n-1\n\n\n-0.7843137\n\n\n\n\n\n\n",
    "preview": "texts/2021-02-25-parks-and-recreation-text-analysis/a3_task3_Khanjian_Roupen_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-25T16:50:31-08:00",
    "input_file": {}
  }
]
